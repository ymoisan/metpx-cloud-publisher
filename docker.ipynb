{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cells in the Python REPL inside the container, e.g. after\n",
    "\n",
    "`docker run -it [your_image] /bin/bash`\n",
    "\n",
    "> Tip : if you get disconnected, get the container-id from a `docker ps` and \n",
    "\n",
    "`docker exec -it <container-id> bash`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import getpass # Not allowed on SSC DataHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing uploading a single file on S3 manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ACCESS_KEY = getpass.getpass(prompt='ACCESS_KEY: ')\n",
    "SECRET_KEY = getpass.getpass(prompt='SECRET_KEY: ')\n",
    "\n",
    "# Using an existing S3 bucket for tests\n",
    "bucket_name = \"eccc-msc-bom-out\"\n",
    "\n",
    "client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "# Slightly modified from https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html#uploading-files\n",
    "\n",
    "def upload_file(client, file_name, bucket_name, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket_name: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_name)\n",
    "\n",
    "    # Upload the file\n",
    "    try:\n",
    "        response = client.upload_file(file_name, bucket_name, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# `client` must be configured properly; if you edited the 'keys' cell above and re-ran it, you're toast :-)\n",
    "\n",
    "# >>> os.getcwd() # WORKDIR in container as per Dockerfile\n",
    "# '/sarra'\n",
    "# >>> filename = \"requirements.txt\" # test uploading that file\n",
    "upload_file(client, filename, bucket_name, object_name=\"Test_file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Testing uploading a single file on S3 via `metpx-cloud-publisher.publish_to_s3`. No `sr_subscribe` yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# `import metpx-cloud-publisher` will not work because [hyphens](https://stackoverflow.com/questions/65930543/importing-a-python-custom-package-with-hyphen)\n",
    "#import importlib\n",
    "\n",
    "importlib.import_module('metpx-cloud-publisher') # will not work\n",
    "# Traceback (most recent call last):\n",
    "# ...\n",
    "# File \"/sarra/metpx-cloud-publisher.py\", line 154, in <module>\n",
    "#    event = MetPXCloudPublisher(self)  # noqa\n",
    "# NameError: name 'self' is not defined\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
